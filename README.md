# ğŸ PyWalker

**PyWalker** is a Python toolset for indexing and querying codebases using OpenAI embeddings and FAISS vector search.

It scans a Python project, extracts all functions (with docstrings), generates embeddings using `text-embedding-3-small`, and stores them in a searchable vector database. You can then ask natural language questions about the codebase and retrieve relevant code + answers powered by GPT-4o.

---

## ğŸš€ Features

- âœ… Parse Python functions with [`jedi`](https://github.com/davidhalter/jedi)  
- âœ… Generate embeddings with OpenAI's `text-embedding-3-small`  
- âœ… Store vectors in `FAISS` for local, fast semantic search  
- âœ… Ask questions using GPT-4o with retrieval-augmented context  
- âœ… Supports multiple codebases via subdirectory indexing

---

## ğŸ“¦ Requirements

```bash
pip install openai faiss-cpu jedi numpy
```

Set your OpenAI API key:

```bash
export OPENAI_API_KEY=sk-...
```

---

## ğŸ“‚ Project Structure

```
pywalker/
â”œâ”€â”€ walker.py       # Indexes a Python codebase
â”œâ”€â”€ query.py        # Searches and queries an indexed codebase
â””â”€â”€ vector/         # Contains FAISS indices and metadata per codebase
    â””â”€â”€ fastapi/
        â”œâ”€â”€ faiss.index
        â””â”€â”€ metadata.jsonl
```

---

## ğŸ› ï¸ Usage

### 1. Clone a Python Project

```bash
git clone https://github.com/fastapi/fastapi.git
```

### 2. Index the Project

```bash
python walker.py ./fastapi
```

This creates a folder at `./vector/fastapi/` containing `faiss.index` and `metadata.jsonl`.

### 3. Ask a Question

```bash
python query.py "How does FastAPI define routes?" --codebase fastapi --answer
```

Use `--answer` to get a GPT-4o-generated response based on the most relevant code snippets.

---

## ğŸ§  Example Queries

```text
What operations are supported on quaternions in sympy?
How does FastAPI handle dependency injection?
Where is the __mul__ method defined in the Quaternion class?
```

---

## ğŸ“Œ Notes

- Embedding model used: `text-embedding-3-small` (cheap + fast)
- GPT model used: `gpt-4o` (or switch to `gpt-3.5-turbo`)
- Vector search: `FAISS.IndexFlatL2`
- Output files per project: `faiss.index` + `metadata.jsonl`